{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Stage A Results Notebook \u2014 vMF Time-Series Features\n\nThis notebook is **for reporting + visualization only**.\n\n**Workflow**\n1. Run the Stage A pipeline from the repo root:\n   ```bash\n   python run_vmf_pipeline.py\n   ```\n2. Confirm the output file exists:\n   - `outputs/vmf_subject_features.csv`\n3. Then use this notebook to:\n   - inspect the extracted features\n   - generate figures\n   - summarize relationships with `attention`, `p_factor`, etc.\n\n> Important: This notebook **does not** re-run the feature extraction pipeline.\n> If you change feature logic, edit the Python files and re-run `run_vmf_pipeline.py`.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0. Setup"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import os\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Repo root assumptions:\n# - You open this notebook from notebooks/ directory\n# - The pipeline output lives at ../outputs/vmf_subject_features.csv\n\nOUTPUT_CSV = Path(\"..\") / \"outputs\" / \"vmf_subject_features.csv\"\nOUTPUT_CSV\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "if not OUTPUT_CSV.exists():\n    raise FileNotFoundError(\n        f\"Missing output file: {OUTPUT_CSV}\\n\\n\"\n        \"Run Stage A first from the repo root:\\n\"\n        \"  python run_vmf_pipeline.py\\n\"\n    )\nprint(f\"[OK] Found: {OUTPUT_CSV}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Load Stage A outputs"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "df = pd.read_csv(OUTPUT_CSV)\nprint(\"Shape:\", df.shape)\ndf.head()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "df.describe(include=\"all\").T.head(30)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Quick sanity checks"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Required columns (minimum expected)\nrequired_cols = [\"subject\"]\nmissing = [c for c in required_cols if c not in df.columns]\nif missing:\n    raise ValueError(f\"Missing required columns: {missing}\")\n\n# Identify likely target columns (may or may not exist)\ntargets = [c for c in [\"attention\", \"p_factor\", \"internalizing\", \"externalizing\"] if c in df.columns]\nprint(\"Targets found:\", targets)\n\n# Identify feature columns (numeric, excluding subject + targets)\nexclude = set([\"subject\"]) | set(targets)\nfeature_cols = [c for c in df.columns if c not in exclude and pd.api.types.is_numeric_dtype(df[c])]\n\nprint(f\"# features: {len(feature_cols)}\")\nprint(\"Example features:\", feature_cols[:10])\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Missingness summary"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Missingness per column (top 20)\nmiss = df.isna().mean().sort_values(ascending=False)\nmiss.head(20)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Distributions of key dynamic features"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Choose a few \"core\" features to visualize if present\ncore_feats = [\n    \"entropy_mean\", \"entropy_std\",\n    \"switch_rate\",\n    \"vol_l1\", \"vol_l2\",\n    \"maxp_mean\", \"maxp_std\",\n    \"self_transition_rate\",\n    \"trans_entropy_mean\",\n]\ncore_feats = [c for c in core_feats if c in df.columns]\ncore_feats\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "for c in core_feats:\n    x = df[c].dropna().to_numpy()\n    plt.figure()\n    plt.hist(x, bins=40)\n    plt.title(f\"Histogram: {c}\")\n    plt.xlabel(c)\n    plt.ylabel(\"Count\")\n    plt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Scatter plots: features vs targets\n\nThese plots help interpret which vMF dynamics relate to outcomes.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def scatter_feature_vs_target(feature: str, target: str):\n    sub = df[[feature, target]].dropna()\n    if len(sub) < 5:\n        print(f\"[SKIP] Not enough data for {feature} vs {target}\")\n        return\n\n    plt.figure()\n    plt.scatter(sub[feature], sub[target])\n    plt.xlabel(feature)\n    plt.ylabel(target)\n    plt.title(f\"{feature} vs {target} (n={len(sub)})\")\n    plt.show()\n\n# Try core features vs each target\nfor target in targets:\n    for feature in core_feats:\n        scatter_feature_vs_target(feature, target)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Correlations (numeric features)\n\nThis section computes correlations between:\n- dynamic features\n- and each target\n\nWe report the **top absolute correlations** for interpretability.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\ncorr = df[numeric_cols].corr(numeric_only=True)\ncorr.shape\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def top_corr_with_target(target: str, k: int = 20):\n    if target not in corr.columns:\n        print(f\"[SKIP] {target} not found in numeric columns.\")\n        return\n    s = corr[target].drop(labels=[target]).dropna().abs().sort_values(ascending=False)\n    top = s.head(k)\n    out = pd.DataFrame({\n        \"feature\": top.index,\n        \"abs_corr\": top.values,\n        \"corr\": [corr.loc[f, target] for f in top.index],\n    })\n    return out\n\nfor target in targets:\n    print(f\"\\nTop correlations with {target}:\")\n    display(top_corr_with_target(target, k=15))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Simple baselines (optional)\n\n`run_vmf_pipeline.py` already prints cross-validated performance.\n\nThis section provides a quick *in-notebook* baseline fit (train/test split) for sanity.\nIt is **not** a substitute for the script\u2019s grouped cross-validation.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from numpy.random import default_rng\n\ndef train_test_baseline(target: str, test_frac: float = 0.2, seed: int = 123):\n    if target not in df.columns:\n        print(f\"[SKIP] target {target} not in df.\")\n        return\n\n    sub = df.dropna(subset=[target]).copy()\n    # Select numeric features only\n    exclude = {\"subject\", target} | set([t for t in [\"attention\", \"p_factor\", \"internalizing\", \"externalizing\"] if t in sub.columns])\n    feat_cols = [c for c in sub.columns if c not in exclude and pd.api.types.is_numeric_dtype(sub[c])]\n\n    X = sub[feat_cols].to_numpy(dtype=float)\n    y = sub[target].to_numpy(dtype=float)\n\n    # Standardize features\n    mu = X.mean(axis=0)\n    sd = X.std(axis=0)\n    sd = np.where(sd < 1e-12, 1.0, sd)\n    Xz = (X - mu) / sd\n\n    rng = default_rng(seed)\n    idx = np.arange(len(y))\n    rng.shuffle(idx)\n    n_test = max(1, int(test_frac * len(y)))\n    te = idx[:n_test]\n    tr = idx[n_test:]\n\n    # Ridge with intercept, no sklearn\n    alpha = 10.0\n    Xtr = np.column_stack([np.ones(len(tr)), Xz[tr]])\n    Xte = np.column_stack([np.ones(len(te)), Xz[te]])\n    p = Xtr.shape[1]\n    I = np.eye(p); I[0,0] = 0.0\n    coef = np.linalg.solve(Xtr.T @ Xtr + alpha*I, Xtr.T @ y[tr])\n    yhat = Xte @ coef\n\n    ss_res = float(np.sum((y[te] - yhat)**2))\n    ss_tot = float(np.sum((y[te] - np.mean(y[te]))**2))\n    r2 = 1.0 - ss_res/ss_tot if ss_tot > 1e-12 else 0.0\n\n    plt.figure()\n    plt.scatter(y[te], yhat)\n    plt.xlabel(f\"True {target} (test)\")\n    plt.ylabel(f\"Predicted {target} (test)\")\n    plt.title(f\"Quick ridge baseline (not grouped CV) \u2014 R2={r2:.3f}\")\n    plt.show()\n\n    print(f\"[Baseline] target={target}, n_test={len(te)}, R2={r2:.3f}\")\n\nfor target in targets:\n    train_test_baseline(target)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8. Notes for students\n\n- If you want to change **how features are computed**, edit `vmf_features.py`, then re-run:\n  ```bash\n  python run_vmf_pipeline.py\n  ```\n- If you want to change **which files are loaded**, edit `config.py` or `vmf_dataset.py`.\n- The notebook should remain focused on **plots and interpretation**, not the pipeline.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}